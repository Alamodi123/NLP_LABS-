{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUfbPI0aA68XtI6P6gRH0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alamodi123/Alamodi123/blob/main/TEXT_SUMMARIZATION_LAB_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Extractive Summarization"
      ],
      "metadata": {
        "id": "1b34Ws4akqaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-1 Import Libraries and define input**"
      ],
      "metadata": {
        "id": "vOEWmSXxlEMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Example sentences\n",
        "sentences = [\n",
        " \"Artificial intelligence is transforming industries.\",\n",
        " \"Applications of AI include healthcare, finance, and education.\",\n",
        " \"AI improves efficiency but raises ethical concerns like privacy.\",\n",
        " \"Healthcare benefits from AI in diagnostics and patient care.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "RZNwqiJ9ks4q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-2 Define function to calculate Cosine Similarity Matrix**"
      ],
      "metadata": {
        "id": "OpYk7qV-lMiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate cosine similarity matrix\n",
        "def build_similarity_matrix(sentences):\n",
        " vectorizer = TfidfVectorizer()\n",
        " tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        " similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        " return similarity_matrix"
      ],
      "metadata": {
        "id": "_nNyPXe6k67j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-3 Define function for TextRank Algorithm**"
      ],
      "metadata": {
        "id": "IucrZFWcledf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for TextRank Algorithm\n",
        "def textrank(sentences,similarity_matrix,damping=0.85,max_iter=100, tol=1e-4):\n",
        "  G = nx.Graph()\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "      if i != j:\n",
        "         G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
        "  scores = nx.pagerank(G, alpha=damping, max_iter=max_iter, tol=tol)\n",
        "  ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)),\n",
        " reverse=True)\n",
        "  return ranked_sentences"
      ],
      "metadata": {
        "id": "-lkrz3WGlR38"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-4 Build Similarity Matrix**"
      ],
      "metadata": {
        "id": "V47W_mwfl7ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build similarity matrix\n",
        "similarity_matrix = build_similarity_matrix(sentences)"
      ],
      "metadata": {
        "id": "fefMiN3Rl_8-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-5 Apply TextRank**"
      ],
      "metadata": {
        "id": "zSYqEnIPmXPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TextRank\n",
        "textrank_results = textrank(sentences, similarity_matrix)"
      ],
      "metadata": {
        "id": "HiVJZxLhmZdR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-6 Display the result**"
      ],
      "metadata": {
        "id": "5gdDIQFOmh2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print TextRank Results (All sentences with their scores)\n",
        "print(\"TextRank Sentence Scores:\")\n",
        "for score, sentence in textrank_results:\n",
        " print(f\"Score: {score:.4f}, Sentence: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv43cL8-mfvz",
        "outputId": "6046f766-b0e4-4d58-d7d5-d46a6efd9ed9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextRank Sentence Scores:\n",
            "Score: 0.3934, Sentence: Applications of AI include healthcare, finance, and education.\n",
            "Score: 0.3882, Sentence: Healthcare benefits from AI in diagnostics and patient care.\n",
            "Score: 0.1707, Sentence: AI improves efficiency but raises ethical concerns like privacy.\n",
            "Score: 0.0476, Sentence: Artificial intelligence is transforming industries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1-7 Implement LexRank**"
      ],
      "metadata": {
        "id": "cH6mWwVFmnfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for LexRank Algorithm\n",
        "def lexrank(sentences, similarity_matrix, threshold=0.01):\n",
        "    \"\"\"\n",
        "    Run LexRank on a set of sentences using a similarity matrix.\n",
        "    Returns:\n",
        "      - scores: dict[node_index -> score]\n",
        "      - ranked_sentences: list of (score, sentence) sorted high → low\n",
        "    \"\"\"\n",
        "    n = len(sentences)\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Build the graph with thresholding and row normalization\n",
        "    for i in range(n):\n",
        "        # Count how many neighbours exceed the threshold\n",
        "        row_sum = sum(similarity_matrix[i][j] > threshold for j in range(n))\n",
        "        for j in range(n):\n",
        "            if i != j and similarity_matrix[i][j] > threshold and row_sum > 0:\n",
        "                # Normalize by number of valid edges (simple version)\n",
        "                weight = similarity_matrix[i][j] / row_sum\n",
        "                G.add_edge(i, j, weight=weight)\n",
        "\n",
        "    # Compute PageRank on this directed graph\n",
        "    scores = nx.pagerank(G, max_iter=100, tol=1e-6)\n",
        "\n",
        "    # Build ranked sentence list\n",
        "    ranked_sentences = sorted(\n",
        "        ((scores[node], sentences[node]) for node in scores),\n",
        "        reverse=True\n",
        "    )\n",
        "    return scores, ranked_sentences\n",
        "\n",
        "# Build similarity matrix (again for clarity)\n",
        "similarity_matrix = build_similarity_matrix(sentences)\n",
        "\n",
        "# Apply LexRank\n",
        "lexrank_scores, lexrank_results = lexrank(sentences, similarity_matrix)\n",
        "\n",
        "# Print all LexRank scores\n",
        "print(\"LexRank Sentence Scores (All Sentences):\")\n",
        "for node, score in lexrank_scores.items():\n",
        "    print(f\"Sentence {node + 1}: Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nLexRank Ranked Sentences:\")\n",
        "for score, sentence in lexrank_results:\n",
        "    print(f\"Score: {score:.4f} | Sentence: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPXXiGNgmlCl",
        "outputId": "10043887-597e-4d9f-951b-24ecd83c4678"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LexRank Sentence Scores (All Sentences):\n",
            "Sentence 2: Score: 0.4130\n",
            "Sentence 3: Score: 0.1793\n",
            "Sentence 4: Score: 0.4077\n",
            "\n",
            "LexRank Ranked Sentences:\n",
            "Score: 0.4130 | Sentence: Applications of AI include healthcare, finance, and education.\n",
            "Score: 0.4077 | Sentence: Healthcare benefits from AI in diagnostics and patient care.\n",
            "Score: 0.1793 | Sentence: AI improves efficiency but raises ethical concerns like privacy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Abstractive Summarization\n"
      ],
      "metadata": {
        "id": "Vj9MGgPQm9k6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-1 Import Libraries and Define the Source Text and Target Text**"
      ],
      "metadata": {
        "id": "RsZi0kJVm9aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Sample data: input sentence and target summary\n",
        "source_text = [\"artificial intelligence is transforming industries\"]\n",
        "target_text = [\"ai transforms industries\"]\n",
        "\n",
        "# Vocabulary\n",
        "vocab = [\"<pad>\", \"<sos>\", \"<eos>\", \"artificial\", \"intelligence\", \"is\",\n",
        "         \"transforming\", \"industries\", \"ai\", \"transforms\"]\n",
        "\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Convert sentences to indices\n",
        "def tokenize(text, word2idx):\n",
        "    return [\n",
        "        [word2idx[\"<sos>\"]] +\n",
        "        [word2idx[word] for word in sentence.split()] +\n",
        "        [word2idx[\"<eos>\"]]\n",
        "        for sentence in text\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "qa6shRy3m53D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-2 Converting to PyTorch Tensors**"
      ],
      "metadata": {
        "id": "fhWn7jJHndNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_indices = tokenize(source_text, word2idx)\n",
        "target_indices = tokenize(target_text, word2idx)\n",
        "# Convert to tensors\n",
        "source_tensor = torch.tensor(source_indices, dtype=torch.long)\n",
        "target_tensor = torch.tensor(target_indices, dtype=torch.long)"
      ],
      "metadata": {
        "id": "fTmk0WMBnXmF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-3 Define Hyperparameters**"
      ],
      "metadata": {
        "id": "F22l2kXwnnBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 16\n",
        "hidden_dim = 32\n",
        "vocab_size = len(vocab)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "3Hzo_EpYni97"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-4 Define Encoder**"
      ],
      "metadata": {
        "id": "9LqmUZb8oIWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "JtyaLA14nzEv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-5 Define Decoder**"
      ],
      "metadata": {
        "id": "4CUR3q4CohcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(1)  # Add batch dimension\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n"
      ],
      "metadata": {
        "id": "2SCY_trVonLG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-6 Define Seq2seq Model**"
      ],
      "metadata": {
        "id": "Y2qg-DjEo6FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[:, 0]  # <sos> token\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            x = target[:, t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "rNWj_JmJonzW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-7 Instantiate the Encoder and Decoder**"
      ],
      "metadata": {
        "id": "5hdlDZXLpKOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "decoder = Decoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n"
      ],
      "metadata": {
        "id": "-yXfSuffpHNA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-8 Define the Loss Function and Optimizer**"
      ],
      "metadata": {
        "id": "gtkZkZ7MpSfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Uh_rDd89pQmn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-9 Perform Training**"
      ],
      "metadata": {
        "id": "ETWbCqAtpgdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    source = source_tensor.to(device)\n",
        "    target = target_tensor.to(device)\n",
        "\n",
        "    output = model(source, target)\n",
        "\n",
        "    output = output[:, 1:].reshape(-1, vocab_size)\n",
        "    target = target[:, 1:].reshape(-1)\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dah2qVZpXGd",
        "outputId": "39c01e9c-d435-4f47-b942-63c7cba7619d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 2.2553\n",
            "Epoch [20/100], Loss: 2.0710\n",
            "Epoch [30/100], Loss: 1.8601\n",
            "Epoch [40/100], Loss: 1.5271\n",
            "Epoch [50/100], Loss: 1.1688\n",
            "Epoch [60/100], Loss: 0.8561\n",
            "Epoch [70/100], Loss: 0.6127\n",
            "Epoch [80/100], Loss: 0.4375\n",
            "Epoch [90/100], Loss: 0.3182\n",
            "Epoch [100/100], Loss: 0.2384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2-10 Generate Summary**"
      ],
      "metadata": {
        "id": "fVJhnPkTpw1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a summary\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    source = source_tensor.to(device)\n",
        "    hidden, cell = encoder(source)\n",
        "\n",
        "    x = torch.tensor([word2idx[\"<sos>\"]]).to(device)\n",
        "    summary = []\n",
        "\n",
        "    for _ in range(10):\n",
        "        output, hidden, cell = decoder(x, hidden, cell)\n",
        "        x = output.argmax(1)\n",
        "        word = idx2word[x.item()]\n",
        "\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "\n",
        "        summary.append(word)\n",
        "\n",
        "    print(\"Generated Summary:\", \" \".join(summary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHPRGlGHpt9B",
        "outputId": "0df1ad2d-cfa0-485b-8f37-f3c5c481b123"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: ai transforms industries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Laboratory Task"
      ],
      "metadata": {
        "id": "Dr88dO1WqBef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "# Load built-in dataset (Brown Corpus)\n",
        "sentences = [\" \".join(sentence) for sentence in brown.sents(categories='news')[:100]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv1OwS0dp104",
        "outputId": "59e9a591-95a8-4dd8-cbff-e92db657ff51"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.1 – Build Similarity Matrix & Apply TextRank**"
      ],
      "metadata": {
        "id": "B8eE_Frr98p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build similarity matrix for Brown Corpus sentences\n",
        "similarity_matrix = build_similarity_matrix(sentences)\n",
        "\n",
        "# Apply TextRank\n",
        "textrank_results = textrank(sentences, similarity_matrix)\n",
        "\n",
        "# Display TextRank results (sentence scores)\n",
        "print(\"TextRank Sentence Scores:\")\n",
        "for score, sentence in textrank_results[:5]:  # showing only top 5 for readability\n",
        "    print(f\"Score: {score:.4f}, Sentence: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4bZueRpqMNo",
        "outputId": "7158e094-8b8b-4c18-eedf-83bbe9cd6762"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextRank Sentence Scores:\n",
            "Score: 0.0205, Sentence: The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
            "Score: 0.0204, Sentence: `` This is one of the major items in the Fulton County general assistance program '' , the jury said , but the State Welfare Department `` has seen fit to distribute these funds through the welfare departments of all the counties in the state with the exception of Fulton County , which receives none of this money .\n",
            "Score: 0.0181, Sentence: `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
            "Score: 0.0170, Sentence: The jury also commented on the Fulton ordinary's court which has been under fire for its practices in the appointment of appraisers , guardians and administrators and the awarding of fees and compensation .\n",
            "Score: 0.0160, Sentence: The jury did not elaborate , but it added that `` there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable '' .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.1 Apply LexRank**"
      ],
      "metadata": {
        "id": "i1yxWXC2GGdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build similarity matrix (can reuse, but we rebuild here for clarity)\n",
        "similarity_matrix = build_similarity_matrix(sentences)\n",
        "\n",
        "# Apply LexRank\n",
        "lexrank_scores, lexrank_results = lexrank(sentences, similarity_matrix)\n",
        "\n",
        "# Print all LexRank Scores (All sentences with their scores)\n",
        "print(\"LexRank Sentence Scores (All Sentences):\")\n",
        "for node, score in lexrank_scores.items():\n",
        "    print(f\"Sentence {node + 1}: Score: {score:.4f}\")\n",
        "\n",
        "# Print LexRank Summary (Ranked Sentences)\n",
        "print(\"\\nLexRank Summary (Top Ranked Sentences):\")\n",
        "for score, sentence in lexrank_results[:5]:  # showing top 5 to keep output readable\n",
        "    print(f\"Score: {score:.4f}, Sentence: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1d9cMkGGRFN",
        "outputId": "02ff82fa-a1a2-4ed3-a0dd-bc56853a726f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LexRank Sentence Scores (All Sentences):\n",
            "Sentence 1: Score: 0.0113\n",
            "Sentence 2: Score: 0.0207\n",
            "Sentence 3: Score: 0.0120\n",
            "Sentence 4: Score: 0.0183\n",
            "Sentence 5: Score: 0.0119\n",
            "Sentence 6: Score: 0.0133\n",
            "Sentence 7: Score: 0.0150\n",
            "Sentence 9: Score: 0.0124\n",
            "Sentence 10: Score: 0.0110\n",
            "Sentence 11: Score: 0.0095\n",
            "Sentence 12: Score: 0.0095\n",
            "Sentence 13: Score: 0.0126\n",
            "Sentence 14: Score: 0.0106\n",
            "Sentence 15: Score: 0.0206\n",
            "Sentence 16: Score: 0.0080\n",
            "Sentence 17: Score: 0.0127\n",
            "Sentence 18: Score: 0.0076\n",
            "Sentence 19: Score: 0.0172\n",
            "Sentence 21: Score: 0.0131\n",
            "Sentence 22: Score: 0.0133\n",
            "Sentence 23: Score: 0.0108\n",
            "Sentence 24: Score: 0.0162\n",
            "Sentence 26: Score: 0.0090\n",
            "Sentence 27: Score: 0.0113\n",
            "Sentence 29: Score: 0.0109\n",
            "Sentence 30: Score: 0.0147\n",
            "Sentence 31: Score: 0.0083\n",
            "Sentence 34: Score: 0.0054\n",
            "Sentence 35: Score: 0.0091\n",
            "Sentence 36: Score: 0.0120\n",
            "Sentence 37: Score: 0.0090\n",
            "Sentence 38: Score: 0.0129\n",
            "Sentence 40: Score: 0.0114\n",
            "Sentence 41: Score: 0.0082\n",
            "Sentence 42: Score: 0.0093\n",
            "Sentence 43: Score: 0.0073\n",
            "Sentence 44: Score: 0.0112\n",
            "Sentence 45: Score: 0.0083\n",
            "Sentence 46: Score: 0.0064\n",
            "Sentence 47: Score: 0.0114\n",
            "Sentence 48: Score: 0.0138\n",
            "Sentence 50: Score: 0.0120\n",
            "Sentence 51: Score: 0.0119\n",
            "Sentence 52: Score: 0.0141\n",
            "Sentence 53: Score: 0.0095\n",
            "Sentence 56: Score: 0.0079\n",
            "Sentence 58: Score: 0.0117\n",
            "Sentence 59: Score: 0.0131\n",
            "Sentence 61: Score: 0.0102\n",
            "Sentence 63: Score: 0.0125\n",
            "Sentence 64: Score: 0.0155\n",
            "Sentence 65: Score: 0.0127\n",
            "Sentence 67: Score: 0.0137\n",
            "Sentence 68: Score: 0.0143\n",
            "Sentence 69: Score: 0.0139\n",
            "Sentence 70: Score: 0.0121\n",
            "Sentence 71: Score: 0.0136\n",
            "Sentence 72: Score: 0.0100\n",
            "Sentence 73: Score: 0.0095\n",
            "Sentence 74: Score: 0.0127\n",
            "Sentence 75: Score: 0.0079\n",
            "Sentence 76: Score: 0.0125\n",
            "Sentence 77: Score: 0.0128\n",
            "Sentence 78: Score: 0.0134\n",
            "Sentence 79: Score: 0.0097\n",
            "Sentence 80: Score: 0.0097\n",
            "Sentence 81: Score: 0.0140\n",
            "Sentence 83: Score: 0.0112\n",
            "Sentence 84: Score: 0.0099\n",
            "Sentence 85: Score: 0.0065\n",
            "Sentence 87: Score: 0.0101\n",
            "Sentence 89: Score: 0.0066\n",
            "Sentence 90: Score: 0.0119\n",
            "Sentence 91: Score: 0.0076\n",
            "Sentence 92: Score: 0.0092\n",
            "Sentence 93: Score: 0.0078\n",
            "Sentence 94: Score: 0.0098\n",
            "Sentence 95: Score: 0.0117\n",
            "Sentence 96: Score: 0.0065\n",
            "Sentence 97: Score: 0.0052\n",
            "Sentence 98: Score: 0.0058\n",
            "Sentence 100: Score: 0.0068\n",
            "Sentence 33: Score: 0.0056\n",
            "Sentence 39: Score: 0.0065\n",
            "Sentence 49: Score: 0.0095\n",
            "Sentence 54: Score: 0.0070\n",
            "Sentence 55: Score: 0.0054\n",
            "Sentence 57: Score: 0.0079\n",
            "Sentence 60: Score: 0.0093\n",
            "Sentence 86: Score: 0.0076\n",
            "Sentence 88: Score: 0.0066\n",
            "Sentence 32: Score: 0.0038\n",
            "Sentence 8: Score: 0.0018\n",
            "Sentence 20: Score: 0.0018\n",
            "Sentence 25: Score: 0.0025\n",
            "Sentence 99: Score: 0.0021\n",
            "Sentence 62: Score: 0.0030\n",
            "Sentence 66: Score: 0.0021\n",
            "Sentence 82: Score: 0.0023\n",
            "\n",
            "LexRank Summary (Top Ranked Sentences):\n",
            "Score: 0.0207, Sentence: The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
            "Score: 0.0206, Sentence: `` This is one of the major items in the Fulton County general assistance program '' , the jury said , but the State Welfare Department `` has seen fit to distribute these funds through the welfare departments of all the counties in the state with the exception of Fulton County , which receives none of this money .\n",
            "Score: 0.0183, Sentence: `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
            "Score: 0.0172, Sentence: The jury also commented on the Fulton ordinary's court which has been under fire for its practices in the appointment of appraisers , guardians and administrators and the awarding of fees and compensation .\n",
            "Score: 0.0162, Sentence: The jury did not elaborate , but it added that `` there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable '' .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.2 ROUGE Evaluation**"
      ],
      "metadata": {
        "id": "pO7cs0zAGatU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLtXUQ82GnAu",
        "outputId": "a78aff4b-3ff2-4694-a809-0a05028b9f8c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=74feff934670443a3eecd8c981fcdc3a73a23705dbdadd7a9a2a7bea0779378c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Extract top-ranked sentences as summaries\n",
        "textrank_summary = textrank_results[0][1]\n",
        "lexrank_summary = lexrank_results[0][1]\n",
        "\n",
        "# Choose a reference summary (using first sentence from Brown corpus)\n",
        "reference_summary = sentences[0]\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3'], use_stemmer=True)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "lexrank_scores = scorer.score(reference_summary, lexrank_summary)\n",
        "textrank_scores = scorer.score(reference_summary, textrank_summary)\n",
        "\n",
        "# Print ROUGE comparison\n",
        "print(\"\\nROUGE Scores Comparison:\")\n",
        "\n",
        "print(\"\\nLexRank ROUGE Scores:\")\n",
        "for metric, score in lexrank_scores.items():\n",
        "    print(f\"{metric.upper()} - Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}\")\n",
        "\n",
        "print(\"\\nTextRank ROUGE Scores:\")\n",
        "for metric, score in textrank_scores.items():\n",
        "    print(f\"{metric.upper()} - Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If56blwxGepR",
        "outputId": "6d4820f8-020f-4e14-e1d8-f067c1603491"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores Comparison:\n",
            "\n",
            "LexRank ROUGE Scores:\n",
            "ROUGE1 - Precision: 0.1750, Recall: 0.3043, F1: 0.2222\n",
            "ROUGE2 - Precision: 0.0256, Recall: 0.0455, F1: 0.0328\n",
            "ROUGE3 - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "\n",
            "TextRank ROUGE Scores:\n",
            "ROUGE1 - Precision: 0.1750, Recall: 0.3043, F1: 0.2222\n",
            "ROUGE2 - Precision: 0.0256, Recall: 0.0455, F1: 0.0328\n",
            "ROUGE3 - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of TextRank and LexRank Based on ROUGE Scores**\n",
        "\n",
        "Both TextRank and LexRank produced identical ROUGE scores because both algorithms selected the same top-ranked sentence from the Brown corpus. This happens when the dataset contains short, factual, and structurally similar sentences, causing both graph-based methods to identify the same \"central\" sentence based on TF-IDF similarity.\n",
        "\n",
        "The ROUGE-1 scores (unigram overlap) show moderate similarity to the reference summary, while ROUGE-2 and ROUGE-3 scores are low or zero. This is expected because bigram and trigram matches require exact consecutive word overlap, which is unlikely when comparing single sentences. The low scores do not indicate poor summarization performance; they simply reflect the strictness of ROUGE when used on very short texts.\n",
        "\n",
        "Overall, neither algorithm performed better—both produced the same summary and therefore the same evaluation results. The similarity in output demonstrates that, for this dataset, TextRank and LexRank behave similarly due to the uniform style and limited variation in the input sentences."
      ],
      "metadata": {
        "id": "nODFgiL6HMxl"
      }
    }
  ]
}